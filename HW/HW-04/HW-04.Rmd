---
title: "HW-04"
author: "Sterling Hayden"
date: "2024-09-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(fpp3)
library(fable)
library(fabletools)
library(lubridate)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(tsibble)
library(fable)
library(tidyverse)
library(imputeTS)
```


# Creating tsible data
```{r}
#read in the csv
train <- read.csv('hrl_load_metered.csv')
test1 <- read.csv('hrl_load_metered-test1.csv')
test2 <- read.csv('hrl_load_metered-test2.csv')

#convert to a tsibble
train$datetime_beginning_ept <- as.POSIXct(train$datetime_beginning_ept,
                                              format = "%m/%d/%y %H:%M",
                                              tz = "America/New_York")
#previous weeks testing data
test1$datetime_beginning_ept <- as.POSIXct(test$datetime_beginning_ept,
                                              format = "%m/%d/%y %H:%M",
                                              tz = "America/New_York")
#newer testing data
test$datetime_beginning_ept <- as.POSIXct(test$datetime_beginning_ept,
                                              format = "%m/%d/%y %H:%M",
                                              tz = "America/New_York")
#join the train and test1
train <- rbind(train,test1)

nrow(train)
nrow(test)
```

We have to account for multiple 2:00amâ€™s in the spring. Lets deal with those duplicates
```{r}
#remove duplicates by grouping then calculating the average then ungrouping
train <- train %>%
  group_by(datetime_beginning_ept) %>%
  summarise(
    mw = mean(mw, na.rm = TRUE), #calculate the mean mw for each datetime_beginning_ept
    .groups = 'drop' #ungroup the data
  )

test <- test %>%
  group_by(datetime_beginning_ept) %>%
  summarise(
    mw = mean(mw, na.rm = TRUE),
    .groups = 'drop'
  )
```

We now can convert it to a tsibble.
```{r}
train.ts <- train %>% 
  select(datetime_beginning_ept, mw) %>% 
  as_tsibble(index = datetime_beginning_ept)

test.ts <- test %>% 
  select(datetime_beginning_ept, mw) %>% 
  as_tsibble(index = datetime_beginning_ept)

head(train.ts)
```

We also have to account for the lost 2:00am's in the fall.
```{r}
#fill the missing time gaps with NA's
train.ts <- tsibble::fill_gaps(train.ts)
test.ts <- tsibble::fill_gaps(test.ts)

#impute the NA's with the average between the t-1 and t+1
train.ts <- train.ts %>%
  na_interpolation(option = "spline")
test.ts <- test.ts %>%
  na_interpolation(option = "spline")
```


# Looking into the time series
```{r}
autoplot(train.ts, mw)
```


# Build an appropriate Exponential Smoothing Model. 
Forecast this model for your validation set only. 
Calculate the MAE and MAPE for the validation set. 
```{r}
#create all the different models
mw_fit_ETS <- train.ts |>
  model(
    `SES` = ETS(mw ~ error("A") + trend("N") + season("N")),
    `Linear` = ETS(mw ~ error("A") + trend("A") + season("N")),
    `Damped Linear` = ETS(mw ~ error("A") + trend("Ad") + season("N")),
    `Holt-Winters Additive` = ETS(mw ~ error("A") + trend("A") + season("A")),
    `Holt-Winters' Multiplicative` = ETS(mw ~ error("M") + trend("A") + season("M")),
    `Holt-Winters' Multiplicative Damped` = ETS(mw ~ error("M") + trend("Ad") + season("M")),
    `Additive Trend Multiplicative Seasonality` = ETS(mw ~ error("A") + trend("A") + season("M")),
    `Multiplicative Error Additive Trend & Seasonality` = ETS(mw ~ error("M") + trend("A") + season("A"))

  )

#fc with above models
mw_fc_ets <- mw_fit_ETS |>
  fabletools::forecast(h = nrow(test.ts))

#see how well the different models did on the train + val data
fabletools::accuracy(mw_fc_ets, bind_rows(train.ts, test.ts))
```
Holt-Winters' Multiplicative has the lowest MAPE and MAE.

```{r}
autoplot(mw_fc, test.ts, level = NULL) + # Plot the forecast for val.ts
  autolayer(test.ts, mw, color = "black", size = 1.1) +  # Actual validation data
  labs(title = "All Forecasts Plotted Against The Test Data",
       y = "Hourly MW") +
  guides(colour = guide_legend(title = "Model Type")) 
```
This makes sense why our linear models are so bad.


# Build a seasonal ARIMA model. 
Describe the approach you used to select the lags of the model.  
Forecast this model for your validation set only. 
Calculate the MAE and MAPE for the validation set. 

## PACF and ACF Plots
```{r}
ggAcf(train.ts$mw,lag=25)
ggPacf(train.ts$mw,lag=25)
```
I there are some clear issues here. Lets try and fix it with some differences. 

```{r}
#non-seasonal differences
train.ts %>%
 features(mw, unitroot_ndiffs)
```
We should take 1 non-seasonal difference. What about seasonal differences?

```{r}
#seasonal differences
train.ts %>%
 features(mw, unitroot_nsdiffs)
```
We should take 1 seasonal difference.


## Seasonal differences
If we set stepwise=FALSE and approximation=FALSE, we make R work extra hard to find the best ARIMA model. This takes much longer, but with only one series to model, the extra time taken is not a problem.
```{r}
#fit the following ARIMA model (only need to model one if we set stepwise = FALSE, approx = FALSE)
mw_fit_arima <- train.ts |>
  model(auto = ARIMA(mw))#, stepwise = FALSE, approx = FALSE))


#show the ARIMA models selected
print(mw_fit_arima)

#show the model eval metrics for each ARIMA
glance(mw_fit_arima) |> arrange(AICc) |> select(.model:BIC)
```
ARIMA(2,0,2)(2,1,0)[24]


## Seasonal Dummy Variables
```{r}
#create the hour dummy variable
train_dummy.ts <- train.ts %>%
  mutate(hour = hour(datetime_beginning_ept))
test_dummy.ts <- test.ts %>% 
  mutate(hour = hour(datetime_beginning_ept))
head(train.ts)


#create the model
model_SD_ARIMA <- train_dummy.ts %>%
 model(ARIMA(mw ~ factor(hour) + PDQ(0,0,0)))

#show the report
report(model_SD_ARIMA)
```



## Fourier Transforms
```{r}
model_F_ARIMA <- train.ts %>% 
  model(
  `K = 1` = ARIMA(mw ~ fourier(K=1) + PDQ(0,0,0)),
  `K = 2` = ARIMA(mw ~ fourier(K=2) + PDQ(0,0,0)),
  `K = 3` = ARIMA(mw ~ fourier(K=3) + PDQ(0,0,0)),
  `K = 4` = ARIMA(mw ~ fourier(K=4) + PDQ(0,0,0)),
  `K = 5` = ARIMA(mw ~ fourier(K=5) + PDQ(0,0,0)),
  `K = 6` = ARIMA(mw ~ fourier(K=6) + PDQ(0,0,0)),
  `K = 7` = ARIMA(mw ~ fourier(K=7) + PDQ(0,0,0)),
  `K = 8` = ARIMA(mw ~ fourier(K=8) + PDQ(0,0,0)),
  `K = 9` = ARIMA(mw ~ fourier(K=9) + PDQ(0,0,0)),
  `K = 10` = ARIMA(mw ~ fourier(K=10) + PDQ(0,0,0)),
  `K = 11` = ARIMA(mw ~ fourier(K=11) + PDQ(0,0,0)),
  `K = 12` = ARIMA(mw ~ fourier(K=12) + PDQ(0,0,0)),
)

glance(model_F_ARIMA)
```
K=10 has the lowest AICc


# Model Comparisons 

```{r}
mw_fit_all <- train.ts |>
  model(
    Holt_Winters_Multiplicative = ETS(mw ~ error("M") + trend("A") + season("M")),
    Fourier_K10 = ARIMA(mw ~ fourier(K=12) + PDQ(0,0,0)),
    Seasonal_ARIMA = ARIMA(mw ~ pdq(2,0,2) + PDQ(2,1,0)),
    #Seasonal_Dummy_Variables = ARIMA(mw ~ factor(hour) + PDQ(0,0,0))
  )

#fc with above models
mw_fc_all <- mw_fit_all |>
  fabletools::forecast(h = nrow(test.ts))

#see how well the different models did on the train + val data
fabletools::accuracy(mw_fc_all, test.ts)
```

```{r}
modelF <- train.ts %>%
  model(
    `K = 10` = ARIMA(mw ~ fourier(K = 10))
  )
# FORECAST FOR FOURIER
train_fcF <- modelF |>
  fabletools::forecast(h = len(test.ts))
# ACCURACY
fabletools::accuracy(train_fcF, test.ts)
```




